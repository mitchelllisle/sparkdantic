{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#sparkdantic","title":"SparkDantic","text":"<p>1\ufe0f\u20e3 version: 2.6.0</p> <p>\u270d\ufe0f author: Mitchell Lisle</p>"},{"location":"#pyspark-model-conversion-tool","title":"PySpark Model Conversion Tool","text":"<p>This Python module provides a utility for converting Pydantic models to PySpark schemas. It's implemented as a class  named <code>SparkModel</code> that extends the Pydantic's <code>BaseModel</code>.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Conversion from Pydantic model to PySpark schema (<code>StructType</code> or JSON)</li> <li>Type coercion</li> <li>PySpark as an optional dependency</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Without PySpark:</p> <pre><code>pip install sparkdantic\n</code></pre> <p>Note: only JSON schema generation features are available without PySpark installed. If you attempt to use PySpark-dependent schema generation features, SparkDantic will check that a supported version of Pyspark is installed.</p> <p>With PySpark:</p> <pre><code>pip install \"sparkdantic[pyspark]\"\n</code></pre>"},{"location":"#supported-pyspark-versions","title":"Supported PySpark versions","text":"<p>PySpark version <code>3.3.0</code> or higher, up to but not including <code>4.1.0</code></p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#creating-a-new-sparkmodel","title":"Creating a new SparkModel","text":"<p>A <code>SparkModel</code> is a Pydantic model, and you can define one by simply inheriting from <code>SparkModel</code> and defining some fields:</p> <pre><code>from sparkdantic import SparkModel\nfrom typing import List\n\nclass MyModel(SparkModel):\n    name: str\n    age: int\n    hobbies: List[str]\n</code></pre> <p>\u2139\ufe0f <code>Enum</code>s are supported but they must be mixed with either <code>int</code> (<code>IntEnum</code> in Python \u2265 3.10) or <code>str</code> (<code>StrEnum</code>, in Python \u2265 3.11) built-in types:</p> <pre><code>from enum import Enum\n\nclass Switch(int, Enum):\n    OFF = 0\n    ON = 1\n\nclass MyEnumModel(SparkModel):\n    switch: Switch\n</code></pre> <p>\u2139\ufe0f A field can be excluded from the Spark schema using the pydantic's <code>exclude</code> Field attribute. This is useful when e.g. the pydantic model has Spark incompatible fields. Note that <code>exclude</code> is a pydantic field  attribute and not a sparkdantic feature. Setting it will exclude the field from any pydantic serialisation/deserialisation.</p> <p><pre><code>from pydantic import Field\nfrom sparkdantic import SparkModel\nfrom typing import Any\n\nclass MyModel(SparkModel):\n    name: str\n    age: int\n    arbitrary_data: Any = Field(exclude=True)\n</code></pre> Running <code>MyModel.model_spark_schema(exclude_fields=True)</code> should return the following schema:</p> <pre><code>StructType([\n    StructField('name', StringType(), False),\n    StructField('age', IntegerType(), False)\n])\n</code></pre> <p>Calling <code>model_spark_schema</code> without the option raises exception due to incompatible types.</p>"},{"location":"#generating-a-pyspark-schema","title":"Generating a PySpark Schema","text":""},{"location":"#using-sparkmodel","title":"Using <code>SparkModel</code>","text":"<p>Pydantic has existing models for generating JSON schemas (with <code>model_json_schema</code>). With a <code>SparkModel</code> you can  generate a PySpark schema from the model fields using the <code>model_spark_schema()</code> method:</p> <pre><code>spark_schema = MyModel.model_spark_schema()\n</code></pre> <p>Provides this schema:</p> <pre><code>StructType([\n    StructField('name', StringType(), False),\n    StructField('age', IntegerType(), False),\n    StructField('hobbies', ArrayType(StringType(), False), False)\n])\n</code></pre> <p>You can also generate a PySpark-compatible JSON schema from the model fields using the <code>model_json_spark_schema</code> method:</p> <pre><code>json_spark_schema = MyModel.model_json_spark_schema()\n</code></pre> <p>Provides this schema:</p> <pre><code>{\n    \"type\": \"struct\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"type\": \"string\",\n            \"nullable\": False,\n            \"metadata\": {}\n        },\n        {\n            \"name\": \"age\",\n            \"type\": \"integer\",\n            \"nullable\": False,\n            \"metadata\": {}\n        },\n        {\n            \"name\": \"hobbies\",\n            \"type\": {\n                \"type\": \"array\",\n                \"elementType\": \"string\",\n                \"containsNull\": False\n            },\n            \"nullable\": False,\n            \"metadata\": {}\n        }\n    ]\n}\n</code></pre>"},{"location":"#using-pydantic-basemodel","title":"Using Pydantic <code>BaseModel</code>","text":"<p>You can also generate a PySpark schema for existing Pydantic models using the <code>create_spark_schema</code> function:</p> <pre><code>from sparkdantic import create_spark_schema, create_json_spark_schema\n\nclass EmployeeModel(BaseModel):\n    id: int\n    first_name: str\n    last_name: str\n    department_code: str\n\nspark_schema = create_spark_schema(EmployeeModel)\njson_spark_schema = create_json_spark_schema(EmployeeModel)\n</code></pre> <p>\u2139\ufe0f  In addition to the automatic type conversion, you can also explicitly coerce data types to Spark native types by   setting the <code>spark_type</code> attribute in the <code>SparkField</code> function (which extends the Pydantic <code>Field</code> function), like so: <code>SparkField(spark_type=&lt;datatype&gt;)</code>.  <code>datatype</code> accepts <code>str</code> (e.g. <code>\"bigint\"</code>) or <code>pyspark.sql.types.DataType</code> (e.g. <code>LongType</code>).  This is useful when you want to use a specific data type then the one that Sparkdantic infers by default. </p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions welcome! If you would like to add a new feature / fix a bug feel free to raise a PR and tag me (<code>mitchelllisle</code>) as a reviewer. Please setup your environment locally to ensure all styling and development flow is as close to the standards set in this project as possible. To do this, the main thing you'll need is <code>poetry</code>. You should also run <code>make install-dev-local</code> which  will install the <code>pre-commit-hooks</code> as well as install the project locally. PRs won't be accepted without sufficient tests and  we will be strict on maintaining a 100% test coverage.</p> <p>\u2139\ufe0f Note that after you have run <code>make install-dev-local</code> and make a commit we run the test suite as part of the pre-commit  hook checks. This is to ensure you don't commit code that breaks the tests. This will also try and commit changes to  the COVERAGE.txt file so that we can compare coverage in each PR. Please ensure this file is commited with your changes</p> <p>\u2139\ufe0f Versioning: We use <code>bumpversion</code> to maintain the version across various files. If you submit a PR please run bumpversion to the following rules: - <code>bumpversion major</code>: If you are making breaking changes (that is, anyone who already uses this library can no longer rely on existing methods / functionality) - <code>bumpversion minor</code>: If you are adding functionality or features that maintain existing methods and features - <code>bumpversion patch</code>: If you are fixing a bug or making some other small change</p> <p>Note: \u26a0\ufe0f You can ignore bumping the version if you like. I periodically do releases of any dependency updates anyway so if you can wait a couple of days for your code to be pushed to PyPi then just submit the change and I'll make sure it's included in the next release. I'll do my best to make sure it's released ASAP after your PR is merged.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>...</p>"},{"location":"docs/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Sparkdantic<ul> <li>exceptions</li> <li>model</li> <li>utils</li> </ul> </li> </ul>"},{"location":"docs/sparkdantic/exceptions/","title":"exceptions","text":""},{"location":"docs/sparkdantic/exceptions/#sparkdantic.exceptions.SparkdanticImportError","title":"<code>SparkdanticImportError</code>","text":"<p>               Bases: <code>ImportError</code></p> <p>Error importing PySpark. Raised when PySpark is not installed or the installed version is not supported.</p>"},{"location":"docs/sparkdantic/exceptions/#sparkdantic.exceptions.TypeConversionError","title":"<code>TypeConversionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error converting a model field type to a PySpark type</p>"},{"location":"docs/sparkdantic/model/","title":"model","text":""},{"location":"docs/sparkdantic/model/#sparkdantic.model.SparkModel","title":"<code>SparkModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Pydantic BaseModel subclass with model to PySpark schema conversion.</p> <p>Methods:</p> Name Description <code>model_spark_schema</code> <p>Generates a PySpark schema from the model fields.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model.SparkModel.model_ddl_spark_schema","title":"<code>model_ddl_spark_schema(safe_casting=False, by_alias=True, mode='validation', exclude_fields=False)</code>  <code>classmethod</code>","text":"<p>Generates a Pyspark schema DDL String from the model fields.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel or SparkModel</code> <p>The pydantic model to generate the schema from.</p> required <code>safe_casting</code> <code>bool</code> <p>Indicates whether to use safe casting for integer types.</p> <code>False</code> <code>by_alias</code> <code>bool</code> <p>Indicates whether to use attribute aliases (<code>serialization_alias</code> or <code>alias</code>) or not.</p> <code>True</code> <code>mode</code> <code>JsonSchemaMode</code> <p>The mode in which to generate the schema.</p> <code>'validation'</code> <code>exclude_fields</code> <code>bool</code> <p>Indicates whether to exclude fields from the schema. Fields to be excluded should be annotated with <code>Field(exclude=True)</code> field attribute</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated DDL PySpark schema.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model.SparkModel.model_json_spark_schema","title":"<code>model_json_spark_schema(safe_casting=False, by_alias=True, mode='validation', exclude_fields=False)</code>  <code>classmethod</code>","text":"<p>Generates a PySpark JSON compatible schema from the model fields. This operates similarly to <code>pydantic.BaseModel.model_json_schema()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>safe_casting</code> <code>bool</code> <p>Indicates whether to use safe casting for integer types.</p> <code>False</code> <code>by_alias</code> <code>bool</code> <p>Indicates whether to use attribute aliases (<code>serialization_alias</code> or <code>alias</code>) or not.</p> <code>True</code> <code>mode</code> <code>JsonSchemaMode</code> <p>The mode in which to generate the schema.</p> <code>'validation'</code> <code>exclude_fields</code> <code>bool</code> <p>Indicates whether to exclude fields from the schema. Fields to be excluded should be annotated with <code>Field(exclude=True)</code> field attribute</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The generated PySpark JSON schema.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model.SparkModel.model_spark_schema","title":"<code>model_spark_schema(safe_casting=False, by_alias=True, mode='validation', exclude_fields=False)</code>  <code>classmethod</code>","text":"<p>Generates a PySpark schema from the model fields. This operates similarly to <code>pydantic.BaseModel.model_json_schema()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>safe_casting</code> <code>bool</code> <p>Indicates whether to use safe casting for integer types.</p> <code>False</code> <code>by_alias</code> <code>bool</code> <p>Indicates whether to use attribute aliases (<code>serialization_alias</code> or <code>alias</code>) or not.</p> <code>True</code> <code>mode</code> <code>JsonSchemaMode</code> <p>The mode in which to generate the schema.</p> <code>'validation'</code> <code>exclude_fields</code> <code>bool</code> <p>Indicates whether to exclude fields from the schema. Fields to be excluded should be annotated with <code>Field(exclude=True)</code> field attribute</p> <code>False</code> <p>Returns:</p> Type Description <code>StructType</code> <p>pyspark.sql.types.StructType: The generated PySpark schema.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._from_python_type","title":"<code>_from_python_type(type_, metadata, safe_casting=False, by_alias=True, mode='validation', exclude_fields=False)</code>","text":"<p>Converts a Python type to a corresponding PySpark data type.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Type</code> <p>The python type to convert to a PySpark data type.</p> required <code>metadata</code> <code>list</code> <p>The metadata for the field.</p> required <code>safe_casting</code> <code>bool</code> <p>Indicates whether to use safe casting for integer types.</p> <code>False</code> <code>by_alias</code> <code>bool</code> <p>Indicates whether to use attribute aliases or not.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[str, Dict[str, Any]]</code> <p>Union[str, Dict[str, Any]]: The corresponding PySpark data type (dict for complex types).</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._get_alias_by_attr","title":"<code>_get_alias_by_attr(field_info, attr)</code>","text":"<p>Gets the alias value for a specific attribute from a field info object.</p> <p>Parameters:</p> Name Type Description Default <code>field_info</code> <code>FieldInfoUnion</code> <p>The field info object to extract the alias from.</p> required <code>attr</code> <code>str</code> <p>The attribute name to look for (e.g., 'serialization_alias', 'validation_alias').</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The resolved alias string if found, None otherwise.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._get_annotation_or_return_type","title":"<code>_get_annotation_or_return_type(field_info)</code>","text":"<p>Returns the annotation or return type of a field.</p> <p>Parameters:</p> Name Type Description Default <code>field_info</code> <code>FieldInfoUnion</code> <p>The model field info from which to get the annotation or return type. Can be either a regular FieldInfo or ComputedFieldInfo.</p> required <p>Returns:</p> Name Type Description <code>Type</code> <code>Type</code> <p>The annotation type for regular fields or return type for computed fields.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._get_enum_mixin_type","title":"<code>_get_enum_mixin_type(t)</code>","text":"<p>Returns the mixin type of an Enum.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>EnumType</code> <p>The Enum to get the mixin type from.</p> required <p>Returns:</p> Name Type Description <code>MixinType</code> <code>MixinType</code> <p>The type mixed with the Enum.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the mixin type is not supported (int and str are supported).</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._get_field_alias","title":"<code>_get_field_alias(name, field_info, mode='validation')</code>","text":"<p>Returns the field's alias (if defined) or name based on the mode.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The model field name.</p> required <code>field_info</code> <code>FieldInfoUnion</code> <p>The model field info from which to get the alias or name.</p> required <code>mode</code> <code>JsonSchemaMode</code> <p>The mode in which to generate the schema. Defaults to 'validation'.</p> <code>'validation'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The field alias if defined, otherwise the original field name.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._get_metadata","title":"<code>_get_metadata(field_info)</code>","text":"<p>Returns the metadata of a field.</p> <p>Parameters:</p> Name Type Description Default <code>field_info</code> <code>FieldInfoUnion</code> <p>The model field info from which to get the metadata. Can be either a regular FieldInfo or ComputedFieldInfo.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: The metadata list for regular fields, empty list for computed fields.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._get_schema_items","title":"<code>_get_schema_items(model, mode)</code>","text":"<p>Returns the appropriate fields based on the schema mode.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModelOrSparkModel</code> <p>The model to get fields from.</p> required <code>mode</code> <code>JsonSchemaMode</code> <p>The schema generation mode.</p> required <p>Returns:</p> Type Description <code>Iterable[tuple[str, FieldInfoUnion]]</code> <p>Iterable[tuple[str, FieldInfoUnion]]: Iterator of field items.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._get_spark_type","title":"<code>_get_spark_type(t)</code>","text":"<p>Returns the corresponding PySpark data type for a given Python type.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Type</code> <p>The Python type to convert to a PySpark data type.</p> required <p>Returns:</p> Name Type Description <code>DataType</code> <code>str</code> <p>The corresponding PySpark data type.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the type is not recognized in the type map.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._get_union_type_arg","title":"<code>_get_union_type_arg(t)</code>","text":"<p>Returns the inner type from a Union or the type itself if it's not a Union.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Type</code> <p>The Union type to get the inner type from.</p> required <p>Returns:</p> Name Type Description <code>Type</code> <code>Type</code> <p>The inner type or the original type.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._is_base_model","title":"<code>_is_base_model(cls)</code>","text":"<p>Checks if a class is a pydantic.BaseModel.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Type</code> <p>The class to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if it is a subclass, otherwise False.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._is_optional","title":"<code>_is_optional(t)</code>","text":"<p>Determines if a type is optional.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Type</code> <p>The Python type to check for nullability.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>a boolean indicating nullability.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._is_spark_datatype","title":"<code>_is_spark_datatype(t)</code>","text":"<p>Determines if a type is a PySpark DataType.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Type</code> <p>The Python type to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>a boolean indicating if the type is a PySpark DataType.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model._json_type_to_ddl","title":"<code>_json_type_to_ddl(json_type)</code>","text":"<p>Maps JSON schema types to DDL types.</p> <p>Parameters:</p> Name Type Description Default <code>json_type</code> <code>Union[str, Dict[str, Any]]</code> <p>The JSON schema type to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The DDL type representation.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model.create_ddl_spark_schema","title":"<code>create_ddl_spark_schema(model, safe_casting=False, by_alias=True, mode='validation', exclude_fields=False)</code>","text":"<p>Generates a Pyspark schema DDL String from the model fields.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel or SparkModel</code> <p>The pydantic model to generate the schema from.</p> required <code>safe_casting</code> <code>bool</code> <p>Indicates whether to use safe casting for integer types.</p> <code>False</code> <code>by_alias</code> <code>bool</code> <p>Indicates whether to use attribute aliases (<code>serialization_alias</code> or <code>alias</code>) or not.</p> <code>True</code> <code>mode</code> <code>JsonSchemaMode</code> <p>The mode in which to generate the schema.</p> <code>'validation'</code> <code>exclude_fields</code> <code>bool</code> <p>Indicates whether to exclude fields from the schema. Fields to be excluded should be annotated with <code>Field(exclude=True)</code> field attribute</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated DDL PySpark schema.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model.create_json_spark_schema","title":"<code>create_json_spark_schema(model, safe_casting=False, by_alias=True, mode='validation', exclude_fields=False)</code>","text":"<p>Generates a PySpark JSON compatible schema from the model fields. This operates similarly to <code>pydantic.BaseModel.model_json_schema()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel or SparkModel</code> <p>The pydantic model to generate the schema from.</p> required <code>safe_casting</code> <code>bool</code> <p>Indicates whether to use safe casting for integer types.</p> <code>False</code> <code>by_alias</code> <code>bool</code> <p>Indicates whether to use attribute aliases (<code>serialization_alias</code> or <code>alias</code>) or not.</p> <code>True</code> <code>mode</code> <code>JsonSchemaMode</code> <p>The mode in which to generate the schema.</p> <code>'validation'</code> <code>exclude_fields</code> <code>bool</code> <p>Indicates whether to exclude fields from the schema. Fields to be excluded should be annotated with <code>Field(exclude=True)</code> field attribute</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The generated PySpark JSON schema</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model.create_spark_schema","title":"<code>create_spark_schema(model, safe_casting=False, by_alias=True, mode='validation', exclude_fields=False)</code>","text":"<p>Generates a PySpark schema from the model fields.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel or SparkModel</code> <p>The pydantic model to generate the schema from.</p> required <code>safe_casting</code> <code>bool</code> <p>Indicates whether to use safe casting for integer types.</p> <code>False</code> <code>by_alias</code> <code>bool</code> <p>Indicates whether to use attribute aliases (<code>serialization_alias</code> or <code>alias</code>) or not.</p> <code>True</code> <code>mode</code> <code>JsonSchemaMode</code> <p>The mode in which to generate the schema.</p> <code>'validation'</code> <code>exclude_fields</code> <code>bool</code> <p>Indicates whether to exclude fields from the schema. Fields to be excluded should be annotated with <code>Field(exclude=True)</code> field attribute</p> <code>False</code> <p>Returns:</p> Type Description <code>StructType</code> <p>pyspark.sql.types.StructType: The generated PySpark schema.</p>"},{"location":"docs/sparkdantic/model/#sparkdantic.model.json_schema_to_ddl","title":"<code>json_schema_to_ddl(json_schema)</code>","text":"<p>Converts a JSON schema to a DDL string representation matching Spark's format.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>Dict[str, Any]</code> <p>The JSON schema to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The DDL string representation of the schema.</p>"},{"location":"docs/sparkdantic/utils/","title":"utils","text":""},{"location":"docs/sparkdantic/utils/#sparkdantic.utils.MAX_PYSPARK_VERSION","title":"<code>MAX_PYSPARK_VERSION = '4.1.0'</code>  <code>module-attribute</code>","text":"<p>Exclusive maximum version of PySpark supported</p>"},{"location":"docs/sparkdantic/utils/#sparkdantic.utils.MIN_PYSPARK_VERSION","title":"<code>MIN_PYSPARK_VERSION = '3.3.0'</code>  <code>module-attribute</code>","text":"<p>Inclusive minimum version of PySpark required</p>"},{"location":"docs/sparkdantic/utils/#sparkdantic.utils.require_pyspark","title":"<code>require_pyspark()</code>","text":"<p>Raise SparkdanticImportError if PySpark is not installed</p>"},{"location":"docs/sparkdantic/utils/#sparkdantic.utils.require_pyspark_version_in_range","title":"<code>require_pyspark_version_in_range()</code>","text":"<p>Raise SparkdanticImportError if an unsupported version of PySpark is installed</p>"}]}